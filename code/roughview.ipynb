{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b25bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713f7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbbd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3506f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"master\", \"local[4]\").getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbfa731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(r'data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d7f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---+--------+----+-------------------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|day|week_day|hour|               date|          duration|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---+--------+----+-------------------+------------------+\n",
      "|       2| 2019-12-02 10:09:59|  2019-12-02 10:50:00|              2|        17.67|         3|                 N|         233|           1|           2|       69.5|  0.0|    0.0|       0.0|          18|                  0.3|        87.8|                 0.0|  2|       2|  10|2019-12-02 00:00:00|40.016666666666666|\n",
      "|       2| 2019-12-04 05:48:24|  2019-12-04 06:20:24|              2|        17.92|         3|                 N|          48|           1|           1|       67.0|  0.5|    0.0|      12.0|          15|                  0.3|        94.8|                 0.0|  4|       4|   5|2019-12-04 00:00:00|              32.0|\n",
      "|       2| 2019-12-04 12:01:03|  2019-12-04 12:42:07|              1|        15.57|         5|                 N|         161|           1|           1|       60.0|  0.0|    0.5|       0.0|           0|                  0.3|        63.3|                 2.5|  4|       4|  12|2019-12-04 00:00:00| 41.06666666666667|\n",
      "|       2| 2019-12-01 00:07:47|  2019-12-01 00:17:35|              2|         1.78|         1|                 N|         211|           4|           1|        9.0|  0.5|    0.5|      2.56|           0|                  0.3|       15.36|                 2.5|  1|       1|   0|2019-12-01 00:00:00|               9.8|\n",
      "|       1| 2019-12-01 05:19:39|  2019-12-01 05:31:27|              1|          2.5|         1|                 N|          68|           4|           1|       10.5|  3.0|    0.5|       2.2|           0|                  0.3|        16.5|                 2.5|  1|       1|   5|2019-12-01 00:00:00|              11.8|\n",
      "|       2| 2019-12-01 11:59:35|  2019-12-01 12:35:03|              1|         5.47|         1|                 N|         162|           4|           1|       25.5|  0.0|    0.5|       4.0|           0|                  0.3|        32.8|                 2.5|  1|       1|  11|2019-12-01 00:00:00| 35.46666666666667|\n",
      "|       2| 2019-12-01 16:50:13|  2019-12-01 16:59:40|              1|         1.23|         1|                 N|         107|           4|           1|        8.0|  0.0|    0.5|      2.26|           0|                  0.3|       13.56|                 2.5|  1|       1|  16|2019-12-01 00:00:00|              9.45|\n",
      "|       1| 2019-12-01 21:09:52|  2019-12-01 21:20:48|              1|          3.6|         2|                 N|          80|           4|           1|       52.0|  2.5|    0.5|       7.0|           0|                  0.3|        62.3|                 2.5|  1|       1|  21|2019-12-01 00:00:00|10.933333333333334|\n",
      "|       2| 2019-12-01 23:24:15|  2019-12-01 23:39:16|              1|         3.14|         1|                 N|         186|           4|           1|       12.5|  0.5|    0.5|      3.26|           0|                  0.3|       19.56|                 2.5|  1|       1|  23|2019-12-01 00:00:00|15.016666666666667|\n",
      "|       2| 2019-12-02 01:12:55|  2019-12-02 01:29:18|              3|         8.89|         1|                 N|         138|           4|           1|       26.0|  0.5|    0.5|      8.94|           0|                  0.3|       38.74|                 2.5|  2|       2|   1|2019-12-02 00:00:00|16.383333333333333|\n",
      "|       2| 2019-12-02 15:50:57|  2019-12-02 15:58:34|              1|         0.96|         1|                 N|         113|           4|           1|        6.5|  0.0|    0.5|      1.96|           0|                  0.3|       11.76|                 2.5|  2|       2|  15|2019-12-02 00:00:00| 7.616666666666666|\n",
      "|       2| 2019-12-02 17:51:51|  2019-12-02 18:03:34|              1|          1.9|         1|                 N|         170|           4|           1|        9.5|  1.0|    0.5|      3.45|           0|                  0.3|       17.25|                 2.5|  2|       2|  17|2019-12-02 00:00:00|11.716666666666667|\n",
      "|       1| 2019-12-02 20:17:04|  2019-12-02 20:34:03|              4|          1.6|         1|                 N|         107|           4|           2|       12.0|  3.0|    0.5|       0.0|           0|                  0.3|        15.8|                 2.5|  2|       2|  20|2019-12-02 00:00:00|16.983333333333334|\n",
      "|       2| 2019-12-02 23:44:27|  2019-12-02 23:50:25|              1|         1.01|         1|                 N|          79|           4|           1|        6.0|  0.5|    0.5|      2.45|           0|                  0.3|       12.25|                 2.5|  2|       2|  23|2019-12-02 00:00:00| 5.966666666666667|\n",
      "|       2| 2019-12-03 13:28:10|  2019-12-03 13:40:42|              2|         1.06|         1|                 N|         234|           4|           1|        9.0|  0.0|    0.5|       2.0|           0|                  0.3|        14.3|                 2.5|  3|       3|  13|2019-12-03 00:00:00|12.533333333333333|\n",
      "|       2| 2019-12-03 18:30:09|  2019-12-03 18:53:23|              4|          2.8|         1|                 N|         246|           4|           1|       16.0|  1.0|    0.5|      4.06|           0|                  0.3|       24.36|                 2.5|  3|       3|  18|2019-12-03 00:00:00|23.233333333333334|\n",
      "|       2| 2019-12-03 20:05:48|  2019-12-03 20:14:05|              1|          0.7|         1|                 N|          79|           4|           1|        6.5|  0.5|    0.5|      2.06|           0|                  0.3|       12.36|                 2.5|  3|       3|  20|2019-12-03 00:00:00| 8.283333333333333|\n",
      "|       1| 2019-12-03 23:15:01|  2019-12-03 23:22:39|              1|          1.3|         1|                 N|         107|           4|           1|        7.5|  3.0|    0.5|      3.35|           0|                  0.3|       14.65|                 2.5|  3|       3|  23|2019-12-03 00:00:00| 7.633333333333334|\n",
      "|       2| 2019-12-04 00:09:49|  2019-12-04 00:44:09|              2|         9.64|         1|                 N|         162|           4|           1|       31.0|  0.5|    0.5|      6.96|           0|                  0.3|       41.76|                 2.5|  4|       4|   0|2019-12-04 00:00:00|34.333333333333336|\n",
      "|       2| 2019-12-01 00:49:23|  2019-12-01 01:08:53|              2|          4.6|         1|                 N|         230|           7|           1|       17.0|  0.5|    0.5|      4.16|           0|                  0.3|       24.96|                 2.5|  1|       1|   0|2019-12-01 00:00:00|              19.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---+--------+----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()# 每次show（）时间太长了，就跳过了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2f1973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VendorID', 'int'),\n",
       " ('tpep_pickup_datetime', 'timestamp'),\n",
       " ('tpep_dropoff_datetime', 'timestamp'),\n",
       " ('passenger_count', 'int'),\n",
       " ('trip_distance', 'double'),\n",
       " ('RatecodeID', 'int'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('payment_type', 'int'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'int'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'double'),\n",
       " ('day', 'int'),\n",
       " ('week_day', 'int'),\n",
       " ('hour', 'int'),\n",
       " ('date', 'timestamp'),\n",
       " ('duration', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b22e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ID(a,b):\n",
    "    if a>=b:\n",
    "        return 1000*b+a\n",
    "    else :\n",
    "        return 1000*a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdfb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.select('trip_distance','PULocationID','DOLocationID','day','week_day','hour','duration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b457aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1.rdd.map(lambda x:Row(x[0],x[3],x[4],x[5],x[6],set_ID(x[1],x[2]))).toDF(['trip_distance','day','week_day','hour', 'duration','ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f430ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.groupBy('ID').agg({'duration':'mean'}).sort('ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b97ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.withColumnRenamed('avg(duration)','duration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b799c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  ID|          duration|\n",
      "+----+------------------+\n",
      "|1001|  2.36358024691358|\n",
      "|1004| 52.06666666666667|\n",
      "|1012| 88.18333333333334|\n",
      "|1013| 39.99895833333333|\n",
      "|1024| 62.15833333333333|\n",
      "|1025|              96.1|\n",
      "|1033|              67.1|\n",
      "|1036|             49.75|\n",
      "|1039| 41.43333333333333|\n",
      "|1041| 51.22222222222222|\n",
      "|1043| 53.38761904761905|\n",
      "|1045|57.300000000000004|\n",
      "|1048|41.537363834422656|\n",
      "|1050|36.535185185185185|\n",
      "|1065| 33.36666666666667|\n",
      "|1066|             37.95|\n",
      "|1068| 42.40700483091788|\n",
      "|1075|             47.75|\n",
      "|1079|  43.0186274509804|\n",
      "|1083|              57.2|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f627a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=df2.selectExpr('ID%1000 as ID1','duration','ID')\n",
    "T=T.selectExpr('ID1','(ID-ID1)/1000 as ID2','duration')\n",
    "T=T.select('ID1',T['ID2'].astype(IntegerType()),'duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6dc24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|ID1|ID2|          distance|\n",
      "+---+---+------------------+\n",
      "|  1|  1| 1.428148148148148|\n",
      "|  4|  1|             16.23|\n",
      "| 12|  1|             16.95|\n",
      "| 13|  1|15.268750000000002|\n",
      "| 24|  1|26.192500000000003|\n",
      "| 25|  1|             18.93|\n",
      "| 33|  1|             18.88|\n",
      "| 36|  1|             19.85|\n",
      "| 39|  1|             28.23|\n",
      "| 41|  1|24.573333333333334|\n",
      "| 43|  1|19.497714285714284|\n",
      "| 45|  1| 18.73428571428571|\n",
      "| 48|  1| 17.83666666666667|\n",
      "| 50|  1| 17.31222222222222|\n",
      "| 65|  1|            16.855|\n",
      "| 66|  1|              15.3|\n",
      "| 68|  1| 16.65536231884058|\n",
      "| 75|  1|             21.82|\n",
      "| 79|  1| 15.44705882352941|\n",
      "| 83|  1|             20.29|\n",
      "+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0207d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz=spark.read.csv(r'taxi_zones.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac0328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz0=dfz.join(T, T.ID1 == dfz.LocationID, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28cfc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz1=dfz0.select('ID1','ID2','Borough','duration')\n",
    "dfz1=dfz1.withColumnRenamed('Borough','Borough1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e6317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz1=dfz1.join(dfz,dfz1.ID2==dfz.LocationID,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e77027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+------------------+----------+---------+--------------------+------------+\n",
      "|ID1|ID2|               Zone1|          distance|LocationID|  Borough|                Zone|service_zone|\n",
      "+---+---+--------------------+------------------+----------+---------+--------------------+------------+\n",
      "|132| 40|         JFK Airport|21.373728571428575|        40| Brooklyn|     Carroll Gardens|   Boro Zone|\n",
      "|132| 86|         JFK Airport| 8.541727272727274|        86|   Queens|        Far Rockaway|   Boro Zone|\n",
      "|126| 42|         Hunts Point| 4.035500000000001|        42|Manhattan|Central Harlem North|   Boro Zone|\n",
      "|230|193|Times Sq/Theatre ...|3.8778378378378378|       193|   Queens|Queensbridge/Rave...|   Boro Zone|\n",
      "|231|209|TriBeCa/Civic Center|0.9718301982714795|       209|Manhattan|             Seaport| Yellow Zone|\n",
      "|265|265|                  NA| 4.348022181146027|       265|  Unknown|                  NA|         N/A|\n",
      "|114| 66|Greenwich Village...| 3.093392070484581|        66| Brooklyn|  DUMBO/Vinegar Hill|   Boro Zone|\n",
      "|167| 75|  Morrisania/Melrose|  4.17241935483871|        75|Manhattan|   East Harlem South|   Boro Zone|\n",
      "|180| 77|          Ozone Park|               3.6|        77| Brooklyn|East New York/Pen...|   Boro Zone|\n",
      "|114| 74|Greenwich Village...| 6.956942675159236|        74|Manhattan|   East Harlem North|   Boro Zone|\n",
      "|229|127|Sutton Place/Turt...| 9.264761904761905|       127|Manhattan|              Inwood|   Boro Zone|\n",
      "|166| 45| Morningside Heights| 8.716129032258063|        45|Manhattan|           Chinatown| Yellow Zone|\n",
      "|148| 35|     Lower East Side| 7.206666666666667|        35| Brooklyn|         Brownsville|   Boro Zone|\n",
      "|182|163|         Parkchester|11.165000000000001|       163|Manhattan|       Midtown North| Yellow Zone|\n",
      "|209| 33|             Seaport|2.1044102564102563|        33| Brooklyn|    Brooklyn Heights|   Boro Zone|\n",
      "|192|173|     Queensboro Hill|              3.64|       173|   Queens|        North Corona|   Boro Zone|\n",
      "|261| 37|  World Trade Center| 6.415106382978724|        37| Brooklyn|      Bushwick South|   Boro Zone|\n",
      "|264| 75|                  NV| 2.489464285714286|        75|Manhattan|   East Harlem South|   Boro Zone|\n",
      "|106| 79|             Gowanus|             5.301|        79|Manhattan|        East Village| Yellow Zone|\n",
      "| 91| 66|           Flatlands| 9.323636363636362|        66| Brooklyn|  DUMBO/Vinegar Hill|   Boro Zone|\n",
      "+---+---+--------------------+------------------+----------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfz1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c16f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone=dfz1.select('ID1','ID2','Borough','Borough1','duration')\n",
    "df_zone=df_zone.withColumnRenamed('Borough','Borough2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00b0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone=df_zone.sort('ID2','ID1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c64d40dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+---------+------------------+\n",
      "|ID1|ID2|Borough2| Borough1|          duration|\n",
      "+---+---+--------+---------+------------------+\n",
      "|  1|  1|     EWR|      EWR|  2.36358024691358|\n",
      "|  4|  1|     EWR|Manhattan| 52.06666666666667|\n",
      "| 12|  1|     EWR|Manhattan| 88.18333333333334|\n",
      "| 13|  1|     EWR|Manhattan| 39.99895833333333|\n",
      "| 24|  1|     EWR|Manhattan| 62.15833333333333|\n",
      "| 25|  1|     EWR| Brooklyn|              96.1|\n",
      "| 33|  1|     EWR| Brooklyn|              67.1|\n",
      "| 36|  1|     EWR| Brooklyn|             49.75|\n",
      "| 39|  1|     EWR| Brooklyn| 41.43333333333333|\n",
      "| 41|  1|     EWR|Manhattan| 51.22222222222222|\n",
      "| 43|  1|     EWR|Manhattan| 53.38761904761905|\n",
      "| 45|  1|     EWR|Manhattan|57.300000000000004|\n",
      "| 48|  1|     EWR|Manhattan|41.537363834422656|\n",
      "| 50|  1|     EWR|Manhattan|36.535185185185185|\n",
      "| 65|  1|     EWR| Brooklyn| 33.36666666666667|\n",
      "| 66|  1|     EWR| Brooklyn|             37.95|\n",
      "| 68|  1|     EWR|Manhattan| 42.40700483091788|\n",
      "| 75|  1|     EWR|Manhattan|             47.75|\n",
      "| 79|  1|     EWR|Manhattan|  43.0186274509804|\n",
      "| 83|  1|     EWR|   Queens|              57.2|\n",
      "+---+---+--------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aba6cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.coalesce(1).write.option(\"header\", \"true\").csv(\"Brough.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0981e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|PULocationID|DOLocationID|avg(trip_distance)|\n",
      "+------------+------------+------------------+\n",
      "|           1|           1| 1.428148148148148|\n",
      "|           1|         162|             35.07|\n",
      "|           1|         264|              0.79|\n",
      "|           1|         265|             13.58|\n",
      "|           2|          45|             19.27|\n",
      "|           2|         216|               3.8|\n",
      "|           2|         218|               3.5|\n",
      "|           2|         237|              17.2|\n",
      "|           3|           3|1.0975000000000001|\n",
      "|           3|          32|1.0333333333333334|\n",
      "|           3|          42|             9.145|\n",
      "|           3|          47|               2.8|\n",
      "|           3|          51|              2.29|\n",
      "|           3|          78|3.6999999999999997|\n",
      "|           3|          81|             2.635|\n",
      "|           3|         127|               4.5|\n",
      "|           3|         142|             13.63|\n",
      "|           3|         147|               4.4|\n",
      "|           3|         174|               4.6|\n",
      "|           3|         182|3.3899999999999997|\n",
      "+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df1.groupBy('PULocationID','DOLocationID').agg({'trip_distance':'mean'}).sort('PULocationID','DOLocationID')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c79284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae3f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|PULocationID|DOLocationID|avg(trip_distance)|\n",
      "+------------+------------+------------------+\n",
      "|           1|           1| 1.428148148148148|\n",
      "|           4|           1|             16.23|\n",
      "|          12|           1|             16.95|\n",
      "|          13|           1|15.268750000000002|\n",
      "|          24|           1|26.192500000000003|\n",
      "|          25|           1|             18.93|\n",
      "|          33|           1|             18.88|\n",
      "|          36|           1|             19.85|\n",
      "|          39|           1|             28.23|\n",
      "|          41|           1|24.573333333333334|\n",
      "|          43|           1|19.497714285714284|\n",
      "|          45|           1| 18.73428571428571|\n",
      "|          48|           1| 17.83666666666667|\n",
      "|          50|           1| 17.31222222222222|\n",
      "|          65|           1|            16.855|\n",
      "|          66|           1|              15.3|\n",
      "|          68|           1| 16.65536231884058|\n",
      "|          75|           1|             21.82|\n",
      "|          79|           1| 15.44705882352941|\n",
      "|          83|           1|             20.29|\n",
      "+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.where(df3['DOLocationID']==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c43f20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44308\\3082581073.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'taxi_zone.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "dfz=spark.read.csv(r'taxi_zone.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79017150",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pivot_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18400\\2062160417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hours = df2.pivot_table(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1989\u001b[1;33m                 \u001b[1;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1990\u001b[0m             )\n\u001b[0;32m   1991\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pivot_table'"
     ]
    }
   ],
   "source": [
    "hours = df2.pivot_table(\n",
    "    index='day',\n",
    "    columns='hour',\n",
    "    values='ID',\n",
    "    aggfunc='count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9508b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f74a2ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.coalesce(1).write.option(\"header\", \"true\").csv(\"data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0a11ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:391)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:402)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4046/1514218375.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3999/1007198195.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1925/1962184514.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1591/70266614.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1602/795154898.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1592/1901635149.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18400\\1838372618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    815\u001b[0m         \"\"\"\n\u001b[0;32m    816\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o200.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:391)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:402)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4046/1514218375.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3999/1007198195.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1925/1962184514.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1591/70266614.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1602/795154898.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1592/1901635149.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "df0=df2.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cfbae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
